# -*- coding: utf-8 -*-
"""Week12_VideoSession3_Tasks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y-N3OkVSALKW-PCHSSQPLOFqn12NfzO3

**Task-36A:** Please use the iris dataset for your models this week. You are supposed to use the petal length as the label and the other variables as features. Please provide a model via cross-validation function in which the cv value is 3 and 5 respectively. Please explain the importance of applying cross-validation in modeling in your own words.
"""

import seaborn as sns
iris = sns.load_dataset("iris")
iris.head()

X = iris[["sepal_length","sepal_width","petal_width"]] # Features
y = iris["petal_length"] # Target

# Choose Model
from sklearn.linear_model import LinearRegression
model = LinearRegression()

# Model Validation for k=3 and k=5 
from sklearn.model_selection import cross_val_score
print(cross_val_score(model,X,y,cv=3))
print(cross_val_score(model,X,y,cv=5))
#Explanation : Results were much lower than expected. We can say that the correct model could not be established or the number of features in the data set may be insufficient.

"""**Task-36B:** Please develop two different models regarding the previous task. The first model should be an underfitting model and the second one should be an overfitting model. Then you should explain why these models are said to be underfit and overfit with respect to the concepts such as high bias and high variance."""

import matplotlib.pyplot as plt
import numpy as np

# Choose Model
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_score

# Apply pipline for Polynomial Features and Linear Regression
def PolynomialRegression(degree=2, **kwargs):
    return make_pipeline(PolynomialFeatures(degree),
                         LinearRegression(**kwargs))

# Visualization all features
plt.scatter(X["sepal_length"],y,color="black")
plt.scatter(X["sepal_width"],y,color="red")
plt.scatter(X["petal_width"],y,color="blue")
axis = plt.axis()

# Create Test data for model
X_test = np.linspace(0, 3, 500)[:,None]

# Select petal_width column for train and test model
X1=X["petal_width"].values.reshape(-1,1)

# Visulaziation model line for each three degree
for degree in [1, 3, 5]:
    y_test = PolynomialRegression(degree).fit(X1, y).predict(X_test)
    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))
plt.xlim(0, 8)
plt.ylim(-2, 12)
plt.legend(loc='best');

"""**Task-36C:** Please illustrate the training and validation scores with respect to different degree values just we have examined during the class. Please provide the figures and decide which degree value would be more appropriate in this regard. Please explain your reasoning in sufficient detail. 


"""

import numpy as np
import matplotlib.pyplot as plt

# Apply validati curve
from sklearn.model_selection import validation_curve
degree = np.arange(0, 21)
train_score, val_score = validation_curve(PolynomialRegression(), X, y,param_name="polynomialfeatures__degree", param_range=degree, cv=7)

# Visulaziation train and validation score
plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')
plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')
plt.legend(loc='best')
plt.ylim(0, 1)
plt.xlabel('degree')
plt.ylabel('score');

#Explanation :As can be seen from the graph, the model does not work correctly, but we can say that the degree we can get the best result is 2 or 2.5.

"""**Task-36D:** Please provide a comprehensive grid search by the application of linear regression (polynomial features) with the use of different parameters and run the model with the best parameters and then illustrate it visually. Explain the importance of grid search during model development."""

from sklearn.model_selection import GridSearchCV

# Hyperparameter Find Unit
param_grid = {'polynomialfeatures__degree': np.arange(21),
              'linearregression__fit_intercept': [True, False],
              'linearregression__normalize': [True, False]}

grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)
grid.fit(X1, y) # Apply model with between X1(petal_width) and y. I choose petal_width vfeatures for visualizations
grid.best_params_
model = grid.best_estimator_ # Select best hyperparameters for model


from sklearn.model_selection import cross_val_score
cross_val_score(model,X1,y,cv=5)

# Visualazation model with best values and selected data
plt.scatter(X1.ravel(), y)
lim = plt.axis()
y_test = model.fit(X1, y).predict(X_test)
plt.plot(X_test.ravel(), y_test);
plt.axis(lim);
# Explanation : Grid search runs a lot of tests for different parameters and allows us to easily find the best combination.